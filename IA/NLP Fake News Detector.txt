NLP Fake News Detector

Project Structure
The hands on project on NLP : Fake News Detector is divided into following tasks:

Task #1: Understand the Problem Statement and business case 
Task #2: Import libraries and datasets
Task #3: Perform Exploratory Data Analysis
Task #4: Perform Data Cleaning
Task #5: Visualize the cleaned data
Task #6: Prepare the data by tokenizing and padding
Task #7: Understand the theory and intuition behind Recurrent Neural Networks and LSTM
Task #8: Build and train the model
Task #9: Assess trained model performance


Code : 

#INSTALL AND IMPORT PACKAGES

!pip install --upgrade tensorflow-gpu==2.0 #

!pip install plotly #data visualization - used to perform interactive data visualization
!pip install --upgrade nbformat
!pip install nltk #natural language processing tool
!pip install spacy # spaCy is an open-source software library for advanced natural language processing
!pip install WordCloud #strong visualizations
!pip install gensim # Gensim is an open-source library for unsupervised topic modeling and natural language processing

import nltk
nltk.download('punkt')

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
import nltk
import re
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
# import keras
from tensorflow.keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional
from tensorflow.keras.models import Model
from jupyterthemes import jtplot
jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False) 
# setting the style of the notebook to be monokai theme  
# this line of code is important to ensure that we are able to see the x and y axes clearly
# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them. 

#IMPORT DATASETS

df_true = pd.read_csv("True.csv")
df_fake = pd.read_csv("Fake.csv")

#Indicate how many data samples do we have per class (i.e.: Fake and True)

Il suffit d'appeler les data sets

df_true
df_fake


#List how many Null element are present and the memory usage for each dataframe

df_true.isnull().sum()
df_fake.isnull().sum()

#Find the memory used by each dataset

df_true.info()
df.fake.info()


#Exploratory Data analysis

df_true ['isfake'] = 0 #Ici la variable isfake prend 0 comme valeur si le texte n'est pas faux donc vrai
df_true.head()
df_fake ['isfake'] = 1 #Ici la variable isfake prend 1 comme valeur si le texte est faux
df_fake.head()

# Concatenate Real and Fake News

df = pd.concat([df_true, df_fake]).reset_index(drop = True)
df

#Suppress date column cause not useful for us (otherwise we enter in time series analysis)

df.drop(columns = ['date'], inplace = True)  #Ici on met inplace = True pour que dans la mémoire de la machine, cela soit réeelement effacé. False supposerait que la colonne a été effacée mais pas dans la mémoire de la machine


# combine title and text together

df['original'] = df['title'] + ' ' + df['text']
df.head()

#Let's explore one element in original

df['original'][0] 

# PERFORM DATA CLEANING

# download stopwords

nltk.download("stopwords")

# Obtain additional stopwords from nltk

from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use']) #on peut rajouter certains stopwords qu'on juge ne pas être dans la liste des stopwords de base
stop_words

# Remove stopwords and remove words with 2 or less characters

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:
            result.append(token)
            
    return result


# Apply the function to the dataframe

df['clean'] = df['original'].apply(preprocess) #ici on crée une nouvelle colonne nommée clean contenant le résulatat de l'application de la fonction preprocess

# Show original news

df['original'][0]

# Show cleaned up news after removing stopwords

print(df['clean'][0])

df

# Obtain the total words present in the dataset

list_of_words = []
for i in df.clean:
    for j in i:
        list_of_words.append(j)

list_of_words

len(list_of_words)

# Obtain the total number of unique words

total_words = len(list(set(list_of_words)))
total_words

# join the words into a string

df['clean_joined'] = df['clean'].apply(lambda x: " ".join(x))

df

df['clean_joined'][0]


#Perform Sanity check on the preprocessing stage by visualizing at least 3 sample news

Il s'agit de juste changer l'indexe au niveau des étapes de nettoyage 

#Visualize our cleaned up dataset

df

# plot the number of samples in 'subject'

plt.figure(figsize = (8, 8))
sns.countplot(y = "subject", data = df)

# plot the number of samples in 'isfake'

plt.figure(figsize = (8, 8))
sns.countplot(y = "isfake", data = df)

# plot the word cloud for text that is Real

plt.figure(figsize = (20,20)) 
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(" ".join(df[df.isfake == 1].clean_joined))
plt.imshow(wc, interpolation = 'bilinear')

# plot the word cloud for text that is Fake

plt.figure(figsize = (20,20)) 
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = stop_words).generate(" ".join(df[df.isfake == 0].clean_joined))
plt.imshow(wc, interpolation = 'bilinear')

# length of maximum document will be needed to create word embeddings 

maxlen = -1
for doc in df.clean_joined:
    tokens = nltk.word_tokenize(doc)
    if(maxlen<len(tokens)):
        maxlen = len(tokens)
print("The maximum number of words in any document is =", maxlen)

# visualize the distribution of number of words in a text

import plotly.express as px
fig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined], nbins = 100)
fig.show()


#Preparing data by performing tokenization and padding

Tokenizer allows us to vectorize text corpus by turning each text into a sequence of integers

# split data into test and train 

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size = 0.2)


from nltk import word_tokenize



# Create a tokenizer to tokenize the words and create sequences of tokenized words


tokenizer = Tokenizer(num_words = total_words)
tokenizer.fit_on_texts(x_train)
train_sequences = tokenizer.texts_to_sequences(x_train)
test_sequences = tokenizer.texts_to_sequences(x_test)

print("The encoding for document\n",df.clean_joined[0],"\n is : ",train_sequences[0])



# Add padding can either be maxlen = 4406 or smaller number maxlen = 40 seems to work well based on results


padded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')
padded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post')


for i,doc in enumerate(padded_train[:2]):
     print("The padded encoding for document",i+1," is : ",doc)

#Build and train the model


# Sequential Model

model = Sequential()

# embeddidng layer

model.add(Embedding(total_words, output_dim = 128))
# model.add(Embedding(total_words, output_dim = 240))


# Bi-Directional RNN and LSTM

model.add(Bidirectional(LSTM(128)))

# Dense layers

model.add(Dense(128, activation = 'relu'))
model.add(Dense(1,activation= 'sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
model.summary()

total_words


y_train = np.asarray(y_train)


# train the model


model.fit(padded_train, y_train, batch_size = 64, validation_split = 0.1, epochs = 2)

#assess trained model performance

# make prediction

pred = model.predict(padded_test)

# if the predicted value is >0.5 it is real else it is fake

prediction = []
for i in range(len(pred)):
    if pred[i].item() > 0.5:
        prediction.append(1)
    else:
        prediction.append(0)

# getting the accuracy

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(list(y_test), prediction)

print("Model Accuracy : ", accuracy)

# get the confusion matrix

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(list(y_test), prediction)
plt.figure(figsize = (25, 25))
sns.heatmap(cm, annot = True)

# category dict
category = { 0: 'Fake News', 1 : "Real News"}

